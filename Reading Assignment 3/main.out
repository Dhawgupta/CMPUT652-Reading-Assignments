\BOOKMARK [1][-]{section.1}{Markov Decision Process \(MDP\)}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Return and Episode}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Unifying notation for episodic and continuing tasks}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Policies and Value Functions}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Recursive form of `39`42`"613A``45`47`"603Av\(s\) and `39`42`"613A``45`47`"603Aq\(s,a\)}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Optimal Policies and Value Functions}{section.1}% 6
\BOOKMARK [1][-]{section.2}{Dynamic Programming}{}% 7
\BOOKMARK [2][-]{subsection.2.1}{Policy Evaluation}{section.2}% 8
\BOOKMARK [2][-]{subsection.2.2}{Policy Improvement}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.3}{Policy Iteration}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.4}{Value Iteration}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.5}{Asynchronous DP}{section.2}% 12
\BOOKMARK [2][-]{subsection.2.6}{Generalized Policy Iteration}{section.2}% 13
\BOOKMARK [2][-]{subsection.2.7}{Efficiency of Dynamic Programming}{section.2}% 14
\BOOKMARK [1][-]{section.3}{Proofs}{}% 15
