\documentclass[twocolumn,11pt]{article}
\usepackage[    % Much better micro typography
protrusion=true, % adjust to your eye needs 
expansion=true,
tracking=true,
kerning=true,
spacing=true,
%letterspace=50, % well spaced smallcaps
shrink=40,      % may be 20 or less is good   
factor=1000]    % may be less that 1000 
{microtype} 
\usepackage{dsfont}
\usepackage[margin=1cm,bmargin=1.4cm]{geometry} % little margins 
\usepackage{lipsum} % dummy text
\usepackage[toc,page]{appendix}
\usepackage{setspace} 
\usepackage{hyperref}
\usepackage{mathtools, nccmath}
\setstretch{0.8}      % same as \linespread{.8} 
\usepackage{amsmath,amsthm,amssymb,hyperref}
\usepackage{dsfont}
\usepackage{amsmath}
% \usepackeg{}
\usepackage{color}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\State}{\mathcal{S}}
\DeclareMathOperator{\Action}{\mathcal{A}}
\DeclareMathOperator{\Reward}{\mathcal{R}}
%\DeclareMathOperator{\Re}{\mathds{R}}
\DeclareMathOperator{\w}{\textbf{w}}
\DeclareMathOperator{\state}{\mathcal{S}}
\DeclareMathOperator{\action}{\mathcal{A}}
\DeclareMathOperator{\reward}{\mathcal{R}}\DeclareMathOperator{\I}{\textbf{I}}
\DeclareMathOperator{\qp}{q_\pi} % indicates the the q vlaue function following policy pi
\DeclareMathOperator{\vp}{v_\pi} % value funciton following policy pi
\DeclareMathOperator{\rhp}{\rho_{\pi}} % the probability fgunciton for following policy pi
\DeclareMathOperator{\Ep}{\E_\pi}
\DeclareMathOperator{\bb}{\textbf{b}}
%\DeclareMathOperator{\b}{ \textbf{b} }
\DeclareMathOperator{\A}{ \textbf{A} }
\DeclareMathOperator{\D}{ \textbf{D} }
%\DeclareMathOperator{\P}{ \textbf{P} }
\DeclareMathOperator{\mub}{\textbf{\mu}}
\DeclareMathOperator{\vpd}{v_{\pi'}}
\DeclareMathOperator{\qpd}{q_{\pi'}}
\DeclareMathOperator{\maxa}{\underset{a}{\text{max }}}
\DeclareMathOperator{\Be}{B_\pi} % the bellman operator
\DeclareMathOperator{\x}{\textbf{x}}
\DeclareMathOperator{\argmaxa}{\underset{a}{\text{argmax }}}
\DeclareMathOperator{\VE}{\overline{VE}}
\DeclareMathOperator{\BE}{\overline{BE}}
\DeclareMathOperator{\PBE}{\overline{PBE}}
\DeclareMathOperator{\X}{\textbf{X}}

\DeclareMathOperator{\dynamics}{\sum_{s',r} p(s',r|s,a)}
\DeclareMathOperator{\vw}{v_{\w}}
\DeclareMathOperator{\ns}{|\state|} % number of elements in state space
\DeclareMathOperator{\vpt}{v_{\pi_\theta}}
\makeatletter         
\def\@maketitle{   % custom maketitle 
{\Large \bfseries \color{red} \@title}
{\scshape updated by:} \@author ~ at  \@date \par 
\smallskip \hrule \bigskip }

% custom section 
\renewcommand{\section}{\@startsection
{section}%                   % the name
{2}%                         % the level
{0mm}%                       % the indent
{-0.5\baselineskip}%            % the before skip
{0.5\baselineskip}%          % the after skip
{\bfseries\color{blue}}} % the style

% custom section 
\renewcommand{\subsection}{\@startsection
{subsection}%                   % the name
{1}%                         % the level
{0mm}%                       % the indent
{-0.5\baselineskip}%            % the before skip
{0.5\baselineskip}%          % the after skip
{\bfseries\color{blue}}} % the style

\makeatother


% 
\title{Chapter 6, 7, 12 RL Book \cite{sutton2018reinforcement}}
\author{Dhawal Gupta}
\date{\today}

\begin{document}
\maketitle


\section{Temporal Difference Learning}
TD learning is a combination of Monte Carlo ideas and Dynamic Programming (DP) ideas.  Like Monte Carlo they learn from raw experience, without the model of world, and like  DP they bootstrap and the estimated values. Used a policy evaluation methodology.  It also uses a variation of Generalized Policy Iteration (GPI). 
\subsection{TD Prediction}
\textbf{Monte Carlo (MC)} methods wait until the return of the state and can be written as $V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$ for non stationary case. Whereas on one hand MC have to wait for the episode to end, TD(0) only need one step , TD(0) update is written as  \useshortskip
\begin{align*}
V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\end{align*}
TD error is defined as the error in estimation and  can be written as follows \useshortskip
\begin{align*}
\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\end{align*}, note that $\delta_t $ is available at time $t+1$, and if the state  value function  are not updated during the episode we  can write  Monte Carlo Error in terms of TD.\useshortskip
\begin{align*}
G_t - V(S_t)  = \sum_{k  = t}^{T -1} \gamma^{k  -t}\delta_k
\end{align*}





We try  to estimate the  value function $\vp$ ,  from experience generated by $\pi$. We try to use approximation methods with a weight/adjustable parameter such as $\w \in R^d$ where $d << |\state|$. Change in a single value of  weight  can  change value of multiple states, and change value of one states can change  the value of other state which  may help in generalizing. Function approximation can't augment the state representation with memories of past observations. 
%\textbf{Note : } \textit{The book mention a point on Partial Observability, which seems kindoff conflicting, as given a PO we really wont be able to learn good value function as we don't know if the features missing were actually contributing to the  value function approximation.}
\subsection{Value Function Approximation}
We define  the updates to estimated value function that shift a  state value to a given target value i.e. given state $s$, it  shifts it  value to a target value say $g$, represented as $s \mapsto g$. This can be interpreted as a similar problem to the \textbf{supervised learning (SL) problem }, where given  an input we have to give an  output (in case of real numbers, regression).  So we  can  consider out case where  out input is the state $s$ and the output is the target value $g$. But one underlying issue with this is most of  the SL methods assume data  to be coming from a stationary distribution , whereas  in the case of RL , due to bootstrapping the  the underlying  distribution for the target value keeps on changing.
\subsection{Prediction Objective $(\overline{VE})$}
There was not requirement for an objective function for the case of tabular approximation, as the values eventually converged to the optimal values, without effecting each other independently. 
But in the case of function approximation, improving the accuracy of one states, will make the estimate of other  states  less accurate, so in this  case we need to place an importance over the possible states.
Specifying a state distribution $\mu(s) \geq 0, \sum_s \mu(s) =  1$, tells us how much we care about the error of a given state. In the case of on policy training in continuing tasks the distribution $\mu(s)$ is the  stationary distribution under the policy $\pi$.

%\textbf{Doubt : } \textit{What about the starting state, it may be possible that for  a given  continuing case, we wont have any transition to possible to a subset of states for a given starting state, in that case $\mu(s)$ might not be a proper distribution.} 
In case of episodic tasks this function starts to depend on the distribution of the starting states as well.
We can write the  probability of starting in state as $h(s)$, and we  define $\eta(s)$  as the number  of time steps spent on average in state $s$ in an episode. We can write it as \useshortskip \begin{align*}
\eta(s) = h(s) +  \gamma \sum_{s'} \eta(s') \sum_a \pi(a|s') p(s|a, s')\forall s\in \state
\end{align*} and we  can normalize the values of $\eta(s)$ to get $\mu(s)$. i.e. $\mu(s)  = \tfrac{\eta(s)}{\sum_{s'}\eta(s')} \forall s \in \state$.  The idea goal is to find $\w^*$  which satisfies the following property $\overline{VE}(\w^*) \leq \overline{VE}(\w) \forall \w$,but usually we are only able to obtain the a local optimum where $\overline{VE}(\w^*) \leq \overline{VE}(\w) \forall \w $ in neighbourhood of $\w^*$



\subsection{Stochastic and Semi Gradient Methods}
We  taken an approximation $\hat{v}(s, \w)$ which is differential wrt to $\w$ for all  values of $s$. We update $\w$ at all discrete time steps and we denote $\w_t $ as the weight vector at each time step. In the ideal case we will be observing the following $s \mapsto \vp(s)$ at each  step and  we will taking a gradient for :
\begin{align*}
\w_{t+1}  & \doteq \w_t - \tfrac{1}{2}\alpha \nabla_w \left[ \vp(s) - \hat{v}(S_t, \w_t) \right]^2 \\
& = \w_t - \alpha \nabla_{w_{t}} \left[ \vp(s) - \hat{v}(S_t, \w_t) \right] \nabla_{w_{t}}  \hat{v}(S_t, \w_t)
\end{align*}
This is know as the  \textbf{Stochastic Gradient Descent }as we are doing a GD  step on a single sample. 
where this will be the gradient  in the real direction of descent but sadly we don't have access to the  $\vp(s)$. We turn  to methods which give us $s \mapsto U_t$, where $U_t$ is itself an estimate for the $\vp(S_t)$.  An example of unbiased $U_t$ can be the monte carlo target $U_t \doteq G_t$. Whereas if we get  $U_t$ by bootstrapping, the estimate will be inherently biased to the value of  $\w_t$ at time step t. Also ideally we should be taking the gradient of $U_t$ as well, but we only take the gradient of  the second ter m and hence this technique is know as the \textbf{Semi Gradient Descent}. Their  convergence is not that robust. But they can be used in a continual and on-line fashion where updates can be done in a singe step. $U_t \doteq R + \gamma \hat{v}(S', \w)$
We can rewrite our SGD equation in the form of $\w_{t+1}= \w_t - \alpha \nabla_{w_{t}} \left[ U_t - \hat{v}(S_t, \w_t) \right] \nabla_{w_{t}}  \hat{v}(S_t, \w_t)$. Where if $U_t$ is unbiased ($\E[U_t | S_t = s] = \vp(s)$), then it is guaranteed to converge to the value. We need to use some other way, if we use a unbiased estimator of the $\vp(s)$ we can guarantee that the expected gradient will be in the correct direction , like using Monte Carlo Methods to use $G_t$
A sample trajectory looks like\\

\subsection{Linear  Methods}
Approximation methods which are a  linear function of weight vector $\w$ are called  as linear methods. The state  representation may be a real valued vector $\x(s) \doteq (x_1(s), x_2(s), \ldots , x_d(s))^T$, and written as   \useshortskip
\begin{align*}
\hat{v}(s, \w) &\doteq \w^T\x(s) \\
\nabla_{\w} \hat{v}(s, \w) &= \x(s)
\end{align*}, where the SGD update for  the weights is now  given  by 
\begin{align*}
\w_{t+1} &\doteq  \w_t  +  \alpha \left[ U_t - \hat{v}(S_t, \w_t)\right]\x(S_t) \text{ (taking TD(0) we get)}\\
 &= \w_t + \alpha (R_{t+1} \x_t - \x_t(\x_t - \gamma \x_{t+1})^T\w_t)\\
\E[\w_{t+1}| \w_{t}]  &= w_t + \alpha( \bf b - Aw_t)
\end{align*}
where $ \textbf{b} \doteq \E[R_{t+1}\x_t] \in R^d$ and $\A \doteq \E[\x_t (\x_t  - \gamma \x_{t+1})^T] \in R^d \times R^d$, on convergence the system converges to $\w_{TD} \doteq \A^{-1}\textbf{b}$.
The convergence of the above system is  dependent on the existence of $\A^{-1}$. Proof of convergence is expanded. in the appendix \ref{ap:1}

\section{Linear Value Function  Geometry}
We can now start to decouple the value function approximation from the learning procedure and build a more geometric picture of the same. 
We can start by representing our set of state as an ordered vector i.e. $\state = \{s_1, s_2,  \ldots s_{|\state|} \}$ and the value function can be represent a vector in $|\state|$
dimensions. i.e,$\vp = \left[v(s_1), v(s_2), \ldots, v(s_{|\state|})\right]^T$,
 and typically out vector of weight $ \w \in R^d $ where
 $d \ll  | \state | $. 
Hence we can say  that $v_w$ lies in a lower dimension when compared to $\vp$,
as we can only vary $\w$ in a lower dimension resulting of $v_w$ being restricted in a very small plane. 
\subsection{Distance Metric}
We also define a distance metric for different value function approximations $||v||_\mu^2 \doteq \sum_{s \in \state} \mu(s) v(s)^2$. By which we c an  define the $\overline{VE}(\w) = \parallel v_{\w} - \vp\parallel_\mu^2$. Defining a projection operator which projects the value function from the higher dimension on the space of $\vw$, written as
\begin{align*}
\prod v &\doteq \vw \\
 \text{where} &
\w = \underset{\w \in R^d}{\text{argmin}} \parallel v - \vw\parallel_\mu^2
\end{align*} 

\subsection{Bellman Error $\overline{BE}(w)$}
Using the bellman  error to get an  estimate  how far our approximation $\vw$ is from the actual value function by doing a bootstrap on $\vw$. defined as  
\begin{align*}
\overline{\delta}_{\w}(s) &\doteq \left( \sum_a \pi(a|s) \sum_{s',r} p(s',t|s,a) [r + \gamma \vw (s')]\right) - \vw(s)\\
&= \E_\pi \left[ R_{t+1} +  \gamma \vw(S_{t+1}  - \vw(S_t) | S_t = s,  A_t \sim \pi\right]\\
\overline{BE}(\w) &= \parallel \overline{\delta}_{\w}\parallel_\mu^2
\end{align*}
We can  call this as the  expected value of the  TD(0) error. i.e. $\overline{BE} = \E[TDE]$ and
\subsection{Bellman Operator $\Be$} 
Bellman  Operator defined as $\Be : R^{\ns} \rightarrow R^{\ns}$, to the appropriate value function defined as 
\begin{align*}
(\Be v)(s) &\doteq \sum_a \pi(a|s) \sum_{s',r} p(s',t|s,a) [r + \gamma v(s')]\\
\overline{\delta}_{\w} &= \Be \vw - \vw \text{ (can  be used  to define)}
\end{align*}
having a stationary point at $\vp$ i.e. $\vp = \Be\vp$.


\subsection{Projected Bellman  Error $\PBE(\w)$}
Normally the operator$\Be$ project the  vector $v$, outside the  span of the $\vw$. Hence we use a projection back to the  space of  $\vw$ to be able to make a  possible update to new values of  $\w$. THe projected bellman error vector is given as $\Pi \overline{\delta}_{\vw}$. The \textbf{Projected Bellman Error (PBE)} where  
\begin{align*}
\overline{PBE}(\w) = \parallel \Pi \overline{\delta}_{\w} \parallel_\mu^2
\end{align*}
All these error normally don't converge to the same value 
% \textbf{Note :} \textit{Need  to study how they are  different and what difference are there.} 
 $min \VE $ gives us $\Pi \vp$, $\Be $ operator without projection back will converge to $\vp$, whereas $\BE$ and $\PBE $ are often different, where the projection operator is defined as $\Pi  \doteq \X (\X^T\D\X)^{-1}\X^T\D$.


\section{Policy Gradients Methods}
In these  set  of approached we  directly  try to optimize the policy with/without the use of value functions. 
\begin{align*}
\theta_{t+1} = \theta_t + \alpha\widehat{\nabla_{\theta_t}J(\theta)}
\end{align*}
Is the general update rule of this where $J(\theta)$ is a performance measure.

\subsection{Policy Gradient Theorem}
We have to treat the case of policy gradients differently for the episodic task and the continual task. Considering the episodic task, we define the  performance metric to be the value function under policy $\pi_\theta $ in the starting state i.e.
\begin{align*}
J(\theta) \doteq \vpt(s_0)
\end{align*}.
\textbf{Challenges} : with function approximation  it may be challenging  to ensure policy improvement with  the change in policy parameters, because change in parameters often results in the change in distribution of action in given state, and the change in policy also  causes a change in the distributions of different states,  hence we have  to account for two gradients according to  our instincts , which  should balance and give us a net improvement in our performance metric. The effect of policy change on the distribution of the  states  is particularly unknown without the environment dynamics, that is where the Policy Gradient Theorem  comes to our aid, which proves that the 
\begin{align*}
\nabla J(\theta) \propto \sum_s \mu(s) \sum_a \qp(s, a) \nabla_\pi (a | s, \theta)
\end{align*}
Proof of this can be submitted separately.
which  gives us that the improvement in performance metric is not actually dependent on the gradient of the state  distribution on  the parameters.

\subsection{REINFORCE}
REINFORCE is a Monte Carlo Policy gradient approach ,  where we sample a complete episode and then take our gradients on that  , hence REINFORCE is limited to episodic  tasks then. 
The SGD update for gradient descent is given by 
\begin{align*}
\theta_{t+1} &\doteq \theta_t + \alpha \sum_a \hat{q}(S_t,  a, \w) \nabla \pi(a |S_t, \theta)\\
\end{align*} where $\hat{q}$ is an approximation of $\qp$. Going to the REINFORCE update we get 
\begin{align*}
\nabla J(\theta) &= \E\left[  \sum_a \pi(a|s,\theta) \qp(S_t, a)\dfrac{\nabla \pi(a|S_t,  \theta)}{\pi(a|S_t,  \theta)}\right]\\
&= \E\left[  \qp(S_t, A_t)\dfrac{\nabla \pi(a|S_t,  \theta)}{\pi(a|S_t,  \theta)}\right]\\
&= \E\left[  G_t\dfrac{\nabla \pi(a|S_t,  \theta)}{\pi(a|S_t,  \theta)}\right]\\
\end{align*} 
hence we get  the following update rule for theta 
\begin{align*}
\theta_{t+1}  \doteq \theta_t + \alpha G_t\dfrac{\nabla \pi(a|S_t,  \theta)}{\pi(a|S_t,  \theta)}
\end{align*}
This update is great because the only parameter dependent term we  have over here  is the policy. Also intuitively what this update equation tells us that we change the parameters to increase the probability of action give, and that is scaled by the  expected reward after that action as well , scaled down by the current probability of that action to smoothen the bias towards highly probable actions.
\textbf{REINFORCE} with baseline, we can add a baseline term  dependent on the current state which can  help improve with the variance of the monte carlo estimation, subtracting a baseline doesnt change the expected gradient direction as $\sum_a b(s) \nabla \pi(a|s, \theta) = b(s) \nabla \sum_a \pi(a | s, \theta) = b(s) \nabla 1 = 0$ and we can write the modified update rule as 
\begin{align*}
\theta_{t+1}  \doteq \theta_t + \alpha (G_t - b(S_t))\dfrac{\nabla \pi(a|S_t,  \theta)}{\pi(a|S_t,  \theta)}
\end{align*}. The  basic idea is that the baseline will be high to  when  all actions in a state have a high value to differentiate between good and bad actions similarly, for a state with all actions having a low value the baseline  will also have a low value.
\subsection{Actor - Critic Methods}
When we do bootstrapping on our approximated value function (unlike baseline REINFORCE,which is unbiased) this is biased , it is called as actor critic. 
The update is given by 
\begin{align*}
\theta_{t+1} \doteq \theta_t + \alpha \delta_t \dfrac{\nabla \pi(A_t   | S_t , \theta_t)  }{\pi (A_t | S_t, \theta_t)}
\end{align*} where $\delta = R_{t+1} + \gamma \hat{v}(S_{t+1} , \w) - \hat{v}(S_t,  \w)$ and we use semi gradient methods
\subsection{PG for Continual Task}
The PG theorem holds for the case of continual task , we just have to modify performance metric  which we put as the expected value of reward i.e. $J(\theta) = r(\pi) = \sum_s \mu(s) \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) r$, and we define values of $\vp(s) \doteq \Ep[G_t|S_t = s]$ wrt to differential return
$G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \ldots$, 
\subsection{Policy Parametrization for Continuous Actions}
Using continues probability distribution like  Gaussian  distributions to define the probability over the action space which can we parametrized by say $\theta$. example of using a Gaussian
\begin{align*}
\pi( a | s, \theta )\doteq \dfrac{1}{\sigma(s, \theta) \sqrt{2\pi}}exp(- \dfrac{(a - \mu(s, \theta))^2}{2\sigma(s,\theta)^2})
\end{align*}
where the mean and sigma can be parametrized as linear function over $x(s)$ as $\mu(s,\theta) = \theta_\mu^T \x_\mu(s)$ and $\sigma(a, \theta) = exp(\theta_\sigma^T \x_\sigma(s))$

%\section{Appendix (Please dont  account for this space)}
%\subsection{Proof of Convergence for Linear TD(0)}\label{ap:1}
%Proof of convergence for Linear TD(0)
%We can rewrite  the $\E[\w_{t+1}| \w_t] = (\I - \alpha \A)\w_t + \alpha \bb)$, as we can  see only $\A$ multiplies  with $\w_t $, hence it is important for $ (\I - \alpha \A)$ to converge i.e. scale $\w_t$ to smaller values \textbf{Note :  }\textit{I am not sure Why we would like to scale down the  values of $\w_t$, I need to get this clarified form someone.}. Given that we want that $\A$ having the following decomposition $\A = Z\Sigma Z^T$ , will give us this result for the  first matrix i.e. $(\I - \alpha \A) = Z (\I - \alpha\Sigma) Z^T$,now if  $\A$ will have negative eigen values, subtraction from one will cause the new  eigenvalues  to be greater than 1 which means we will be scaling teh weights to a larger value,  if on the other hand all the eigen values are positive we can set $\alpha$ so as to make the eigen values $(\I - \alpha \A)$ less than 1 and hence will shrink $\w$. Given that we  need prove that A is positive definite for this to  converge.
%\begin{align*}
%\A &= \sum _s \mu(s) \sum_{s'} p_\pi (s' | s) \x(s) (\x(s) - \gamma \x(s'))^T\\
%&= \sum _s \mu(s) \x(s) (\x(s) - \gamma \sum_{s'} p_\pi (s' | s) \x(s'))^T\\
%&= \textbf{X}^T \D (\I - \gamma \textbf{P})\textbf{X}
%\end{align*}
%where X contains the feature  vector for each state in the  row, i.e.  each row is a feature vector for a state s. D is the diagonal matrix  with each diagonal containing the $\mu(s)$ value. \textbf{P} is a $|S| \times |S|$ matrix $\textbf{P}_{i,j} = p_pi(s_j|s_i)$ (i think)
%Proof Continued in future.
%%\subsection{Proof of $\vp(s)$}


I have provided a kinda of  expanded version for policy gradient theorem in the second document.
\onecolumn
\section{Appendix}
This section will contain proof and some extra results that might be helpful


\bibliographystyle{plain}

\bibliography{reference}

\end{document}
