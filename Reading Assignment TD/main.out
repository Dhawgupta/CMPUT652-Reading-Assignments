\BOOKMARK [1][-]{section.1}{Temporal Difference Learning}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{TD Prediction}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Value Function Approximation}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Prediction Objective \(VE\)}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Stochastic and Semi Gradient Methods}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Linear Methods}{section.1}% 6
\BOOKMARK [1][-]{section.2}{Linear Value Function Geometry}{}% 7
\BOOKMARK [2][-]{subsection.2.1}{Distance Metric}{section.2}% 8
\BOOKMARK [2][-]{subsection.2.2}{Bellman Error BE\(w\)}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.3}{Bellman Operator `39`42`"613A``45`47`"603AB}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.4}{Projected Bellman Error `39`42`"613A``45`47`"603APBE\(`39`42`"613A``45`47`"603Aw\)}{section.2}% 11
\BOOKMARK [1][-]{section.3}{Policy Gradients Methods}{}% 12
\BOOKMARK [2][-]{subsection.3.1}{Policy Gradient Theorem}{section.3}% 13
\BOOKMARK [2][-]{subsection.3.2}{REINFORCE}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.3}{Actor - Critic Methods}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.4}{PG for Continual Task}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.5}{Policy Parametrization for Continuous Actions}{section.3}% 17
\BOOKMARK [1][-]{section.4}{Appendix}{}% 18
