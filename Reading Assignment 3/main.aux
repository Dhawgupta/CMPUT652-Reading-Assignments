\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1}Markov Decision Process (MDP)}{1}{section.1}\protected@file@percent }
\newlabel{eq:1}{{1}{1}{Markov Decision Process (MDP)}{equation.1.1}{}}
\newlabel{eq:2}{{2}{1}{Markov Decision Process (MDP)}{equation.1.2}{}}
\newlabel{eq:3}{{3}{1}{Markov Decision Process (MDP)}{equation.1.3}{}}
\newlabel{eq:4}{{4}{1}{Markov Decision Process (MDP)}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Return and Episode}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Unifying notation for episodic and continuing tasks}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Policies and Value Functions}{1}{subsection.1.3}\protected@file@percent }
\newlabel{eq:vpi}{{9}{1}{Policies and Value Functions}{equation.1.9}{}}
\newlabel{eq:qpi}{{10}{1}{Policies and Value Functions}{equation.1.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Recursive form of $\vp  (s)$ and $\qp  (s,a)$}{1}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Optimal Policies and Value Functions}{1}{subsection.1.5}\protected@file@percent }
\newlabel{eq:v_optimal}{{15}{1}{Optimal Policies and Value Functions}{equation.1.15}{}}
\newlabel{eq:q_optimal}{{16}{1}{Optimal Policies and Value Functions}{equation.1.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Dynamic Programming}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Policy Evaluation}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Policy Improvement}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:condition_pit}{{19}{2}{Policy Improvement}{equation.2.19}{}}
\newlabel{eq:policy_improvement}{{21}{2}{Policy Improvement}{equation.2.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Policy Iteration}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Value Iteration}{2}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Proofs}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Proof of $\vp  (s)$}{2}{subsection.3.1}\protected@file@percent }
\citation{sutton2018reinforcement}
\bibstyle{plain}
\bibdata{reference}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Proof of policy Improvement (I have done this in the exercise solutions)}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Notation}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Terms}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Action-value Methods}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Incremental Implementation}{3}{section.5}\protected@file@percent }
\newlabel{eq:incremental}{{27}{3}{Incremental Implementation}{equation.5.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Tracking a Non Stationary Problem}{3}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Optimistic Initial Values}{3}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Upper-Confidence-Bound Action Selection }{3}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Gradient Bandit Algorithm}{3}{section.9}\protected@file@percent }
